{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1e6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04aee4",
   "metadata": {},
   "source": [
    "Les packages ont été installés avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0f147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
    "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
    "    \"unsloth/codellama-34b-bnb-4bit\",\n",
    "    \"unsloth/tinyllama-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
    "    \"unsloth/gemma-2b-bnb-4bit\",\n",
    "]  # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/mistral-7b-bnb-4bit\",  # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb9f7f",
   "metadata": {},
   "source": [
    "Les modèles ont été importés et configurés avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9f96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5b84a",
   "metadata": {},
   "source": [
    "Connexion à Google Drive réussie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7efb0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load your preprocessed data\n",
    "train_data_en = load_dataset(\"json\", data_files=\"drive/train_data_3_en.json\")\n",
    "train_data_fr = load_dataset(\"json\", data_files=\"drive/train_data_3_fr.json\")\n",
    "test_data_fr = load_dataset(\"json\", data_files=\"drive/test_data_3_fr.json\")\n",
    "test_data_en = load_dataset(\"json\", data_files=\"drive/test_data_3_en.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a284d4",
   "metadata": {},
   "source": [
    "Les jeux de données ont été chargés avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8457c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(train_data, lang):\n",
    "    process_datas = []\n",
    "    for section in train_data[\"train\"]:\n",
    "        if lang == \"fr\":\n",
    "            questions = section[\"questions\"]\n",
    "            reponses = section[\"reponses\"]\n",
    "            textes = section[\"textes\"]\n",
    "            for j in range(len(questions)):\n",
    "                combined_text = \"\"\n",
    "                true_case_texte = textes[j][0]\n",
    "                false_case_texte = textes[j][1]\n",
    "                true_reponse = reponses[j][0]\n",
    "                false_reponse = reponses[j][1]\n",
    "                question = questions[j]\n",
    "                process_datas.append({\"instruction\": f\"{question}\", \"input\":f\"{true_reponse}\", \"output\":f\"{true_case_texte}\"})\n",
    "                process_datas.append({\"instruction\": f\"{question}\", \"input\":f\"{false_reponse}\", \"output\":f\"{false_case_texte}\"})\n",
    "        else:\n",
    "            questions = section[\"questions\"]\n",
    "            reponses = section[\"answers\"]\n",
    "            textes = section[\"texts\"]\n",
    "            for j in range(len(questions)):\n",
    "                combined_text = \"\"\n",
    "                true_case_texte = textes[j][0]\n",
    "                false_case_texte = textes[j][1]\n",
    "                true_reponse = reponses[j][0]\n",
    "                false_reponse = reponses[j][1]\n",
    "                question = questions[j]\n",
    "                process_datas.append({\"instruction\": f\"{question}\", \"input\":f\"{true_reponse}\", \"output\":f\"{true_case_texte}\"})\n",
    "                process_datas.append({\"instruction\": f\"{question}\", \"input\":f\"{false_reponse}\", \"output\":f\"{false_case_texte}\"})\n",
    "    return process_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4e6fa6",
   "metadata": {},
   "source": [
    "La fonction de génération de données a été définie avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a792ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "datas = datas + generate_data(train_data_fr, \"fr\")\n",
    "datas = datas + generate_data(train_data_en, \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e891c202",
   "metadata": {},
   "source": [
    "Les données ont été générées avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbb86b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,  # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 1,  # Increase to 2 for larger GPUs like A100/V100/16+GB\n",
    "        gradient_accumulation_steps = 16,  # Increase to 32 for 16GB+ GPUs.\n",
    "        max_steps = 100,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"paged_adamw_8bit\",\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        warmup_ratio = 0.03,\n",
    "        fp16 = False,\n",
    "        bf16 = True,\n",
    "        report_to = \"none\",\n",
    "        output_dir = \"drive/sft-results\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5408d4",
   "metadata": {},
   "source": [
    "L'entraîneur a été configuré avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dbb058",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0538f",
   "metadata": {},
   "source": [
    "L'entraînement a commencé avec succès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc77a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "finetuned_model = FastLanguageModel.from_pretrained(\"drive/sft-results\")\n",
    "qa_pipeline = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer)\n",
    "\n",
    "# Testing the model on an example prompt\n",
    "example_prompt = \"Le locataire peut-il constaté l'état du logement ?\"\n",
    "response = qa_pipeline(example_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184ab88",
   "metadata": {},
   "source": [
    "Le modèle a été affiné et testé avec succès."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
